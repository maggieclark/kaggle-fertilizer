train = read_csv('train.csv')
train <- train %>%
mutate(Female = case_when(Sex=='female' ~ 1,
Sex=='male'~0)) %>%
dplyr::select(!Sex)
# eval data (subset of training)
eval_index = sample(1:750000, 150000)
eval_dataset = train[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
# remaining training data
train_dataset = train[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
test = read_csv('test.csv')
test <- test %>%
mutate(Female = case_when(Sex=='female' ~ 1,
Sex=='male'~0)) %>%
dplyr::select(!Sex)
View(test)
test_dataset = test %>%
dplyr::select(!id) %>%
as.matrix()
View(test_dataset)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=0.3)
yhat = predict(mod, test_dataset, validate_features = TRUE)
cbind(test$id, yhat)
submission = cbind(test$id, yhat)
View(submission)
write.csv(submission, 'mc_submission_5.29_1.csv')
data.frame(id=test$id, Calories=yhat)
submission = data.frame(id=test$id, Calories=yhat)
write.csv(submission, 'mc_submission_5.29_1.csv')
write_csv(submission, 'mc_submission_5.29_1.csv')
yhat<0
which(yhat<0)
library(xgboost)
library(dplyr)
library(readr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
train = read_csv('train.csv')
train <- train %>%
mutate(Female = case_when(Sex=='female' ~ 1,
Sex=='male'~0)) %>%
dplyr::select(!Sex)
set.seed(117)
fold1 <- train %>% slice_sample(n=150000)
remaining1  <- anti_join(train, fold1, by = 'id')
fold2 <- remaining1 %>% slice_sample(n=150000)
remaining2  <- anti_join(remaining1, fold2, by = 'id')
fold3 <- remaining2 %>% slice_sample(n=150000)
remaining3  <- anti_join(remaining2, fold3, by = 'id')
fold4 <- remaining3 %>% slice_sample(n=150000)
remaining4  <- anti_join(remaining3, fold4, by = 'id')
fold5 <- remaining4
rm(remaining1, remaining2, remaining3, remaining4)
folds = list(fold1, fold2, fold3, fold4, fold5)
droprates = c(0.05, 0.1, 0.15, 0.2, 0.3)
dart500_rmsles = c(99,99,99,99,99)
for (f in 1:5){
print(f)
# training data
train = train %>%
anti_join(folds[[f]], by=join_by(id))
train_mat = xgb.DMatrix(data=as.matrix(train[,c(2:7,9)]),
label=as.matrix(train[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
dr = droprates[f]
print(dr)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 500,
objective="reg:squaredlogerror",
rate_drop =  dr)
print('model trained')
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:500)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart500_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
f=1
dart500_rmsles = c(99,99,99,99,99)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
dr = droprates[f]
print(dr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
rate_drop = dr)
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
train = read_csv('train.csv')
train <- train %>%
mutate(Female = case_when(Sex=='female' ~ 1,
Sex=='male'~0)) %>%
dplyr::select(!Sex)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
dr = droprates[f]
print(dr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
rate_drop = dr)
mod$niter
mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
rm(dart500_rmsles)
rm(dr)
rm(droprates)
learning_rates = c(0.1, 0.3, 0.5, 0.7, 0.9)
dart_es15_rmsles = c(99,99,99,99,99)
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1)
learning_rates = c(0.4, 0.5, 0.6, 0.7, 0.8)
for (f in 1:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
learning_rates = c(0.1, 0.2, 0.3, 0.4, 0.6)
dart_es15_rmsles = c(99,99,99,99,99)
for (f in 1:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1,
skip_drop = 0.8)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
dart_es15_rmsles[1]=0.063
for (f in 2:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1,
skip_drop = 0.8)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
library(readr)
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
train = read_csv('train.csv')
View(train)
# categorical features
train %>%
group_by(`Soil Type`) %>%
summarize(n=n())
train %>%
group_by(`Crop Type`) %>%
summarize(n=n())
# outcome
train %>%
group_by(`Fertilizer Name`) %>%
summarize(n=n())
# temperature
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temperature))
# temperature
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temparature))
# humidity
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Humidity))
# moisture
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Moisture))
# temp
# not sure temp is predictive
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temparature))
# nitrogen
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Nitrogen))
# potassium
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Potassium))
# phos
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Phosphorous))
library(xgboost)
library(dplyr)
library(readr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
set.seed(117)
shuffled_indices <- sample(1:750000)
750000/5
c(0, 150000 * 1:5)
group_assignments <- cut(
shuffled_indices,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
shuffled_indices[1:5]
group_assignments[1:5]
group_assignments[1:50]
table(group_assignments)
train$fold <- group_assignments
cut <- cut(1:10, breaks = c(0,4,7), labels=1:3, include.lowest = T, right = T)
cut <- cut(1:10, breaks = c(0,4,7,10), labels=1:3, include.lowest = T, right = T)
cut
group_assignments <- cut(
train$id,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
group_assignments[1:10]
head(shuffled_indices)
group_assignments <- cut(
shuffled_indices,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
View(train_x)
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
View(test_x)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob")
# outcome
train %>%
group_by(`Fertilizer Name`) %>%
summarize(n=n())
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = c(num_class=7))
list(num_class=7)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7,
enable_categorical=True))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7,
enable_categorical=TRUE))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
yhat = predict(mod, test_x, validate_features = TRUE)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
dummies <- model.matrix(~ `Soil Type` + `Crop Type` - 1, # -1 to remove intercept
data = train)
print(dummies)
View(dummies)
train %>%
select(!c(`Soil Type`,`Crop Type`)) %>%
cbind(dummies)
train_raw = read_csv('train.csv')
train = cbind(train_raw, fold = group_assignments)
View(train)
train = train %>%
select(!c(`Soil Type`,`Crop Type`)) %>%
cbind(dummies)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7))
