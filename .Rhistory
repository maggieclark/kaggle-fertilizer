as.matrix()
print('test data complete')
dr = droprates[f]
print(dr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
rate_drop = dr)
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
train = read_csv('train.csv')
train <- train %>%
mutate(Female = case_when(Sex=='female' ~ 1,
Sex=='male'~0)) %>%
dplyr::select(!Sex)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
dr = droprates[f]
print(dr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
rate_drop = dr)
mod$niter
mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
rm(dart500_rmsles)
rm(dr)
rm(droprates)
learning_rates = c(0.1, 0.3, 0.5, 0.7, 0.9)
dart_es15_rmsles = c(99,99,99,99,99)
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1)
learning_rates = c(0.4, 0.5, 0.6, 0.7, 0.8)
for (f in 1:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
learning_rates = c(0.1, 0.2, 0.3, 0.4, 0.6)
dart_es15_rmsles = c(99,99,99,99,99)
for (f in 1:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1,
skip_drop = 0.8)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
dart_es15_rmsles[1]=0.063
for (f in 2:5){
print(f)
# complete training data for this fold
training_folds = train %>%
anti_join(folds[[f]], by=join_by(id))
print('training folds complete')
# eval data for this fold (subset of training)
eval_index = sample(1:600000, 120000)
eval_dataset = training_folds[eval_index,]
eval_mat = xgb.DMatrix(data=as.matrix(eval_dataset[,c(2:7,9)]),
label=as.matrix(eval_dataset[,8]))
print('eval data complete')
# remaining training data
train_dataset = training_folds[-eval_index,]
train_mat = xgb.DMatrix(data=as.matrix(train_dataset[,c(2:7,9)]),
label=as.matrix(train_dataset[,8]))
print('training data complete')
# test data (f)
test_x = folds[[f]] %>%
dplyr::select(!c(id, Calories)) %>%
as.matrix()
print('test data complete')
# model
lr = learning_rates[f]
print(lr)
w = list(train=train_mat, eval=eval_mat)
mod <- xgb.train(booster = 'dart',
data = train_mat,
nrounds = 1000,
watchlist = w,
early_stopping_rounds=15,
objective="reg:squaredlogerror",
learning_rate=lr,
rate_drop = 0.1,
skip_drop = 0.8)
print('model trained')
num_round = mod$best_iteration
yhat = predict(mod, test_x, validate_features = TRUE, iteration_range=0:num_round)
print(sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2)))
dart_es15_rmsles[f]=sqrt(mean((log(1+yhat)-log(1+folds[[f]]$Calories))^2))
}
library(readr)
library(readxl)
library(dplyr)
library(lubridate)
library(ggplot2)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
train = read_csv('train.csv')
View(train)
# categorical features
train %>%
group_by(`Soil Type`) %>%
summarize(n=n())
train %>%
group_by(`Crop Type`) %>%
summarize(n=n())
# outcome
train %>%
group_by(`Fertilizer Name`) %>%
summarize(n=n())
# temperature
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temperature))
# temperature
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temparature))
# humidity
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Humidity))
# moisture
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Moisture))
# temp
# not sure temp is predictive
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Temparature))
# nitrogen
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Nitrogen))
# potassium
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Potassium))
# phos
train %>%
group_by(`Fertilizer Name`) %>%
summarize(avg_temp=mean(Phosphorous))
library(xgboost)
library(dplyr)
library(readr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
set.seed(117)
shuffled_indices <- sample(1:750000)
750000/5
c(0, 150000 * 1:5)
group_assignments <- cut(
shuffled_indices,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
shuffled_indices[1:5]
group_assignments[1:5]
group_assignments[1:50]
table(group_assignments)
train$fold <- group_assignments
cut <- cut(1:10, breaks = c(0,4,7), labels=1:3, include.lowest = T, right = T)
cut <- cut(1:10, breaks = c(0,4,7,10), labels=1:3, include.lowest = T, right = T)
cut
group_assignments <- cut(
train$id,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
group_assignments[1:10]
head(shuffled_indices)
group_assignments <- cut(
shuffled_indices,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
View(train_x)
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
View(test_x)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob")
# outcome
train %>%
group_by(`Fertilizer Name`) %>%
summarize(n=n())
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = c(num_class=7))
list(num_class=7)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7,
enable_categorical=True))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7,
enable_categorical=TRUE))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
yhat = predict(mod, test_x, validate_features = TRUE)
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
enable_categorical=TRUE,
params = list(num_class=7))
dummies <- model.matrix(~ `Soil Type` + `Crop Type` - 1, # -1 to remove intercept
data = train)
print(dummies)
View(dummies)
train %>%
select(!c(`Soil Type`,`Crop Type`)) %>%
cbind(dummies)
train_raw = read_csv('train.csv')
train = cbind(train_raw, fold = group_assignments)
View(train)
train = train %>%
select(!c(`Soil Type`,`Crop Type`)) %>%
cbind(dummies)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
mod <- xgboost(data = train_x,
label = train_y,
nrounds = 10,
objective = "multi:softprob",
params = list(num_class=7))
library(xgboost)
library(dplyr)
library(readr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
train_raw = read_csv('train.csv')
set.seed(117)
shuffled_indices <- sample(1:750000)
group_assignments <- cut(
shuffled_indices,
breaks = c(0, 150000 * 1:5),
labels = 1:5,
include.lowest = TRUE, # include the first value in the first interval
right = TRUE # intervals are (a, b], except for the first if include.lowest = TRUE
)
train = cbind(train_raw, fold = group_assignments)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softprob"),
nrounds = 10)
dummies <- model.matrix(~ `Soil Type` + `Crop Type` - 1, # -1 to remove intercept
data = train)
View(dummies)
View(dummies)
train = train %>%
select(!c(`Soil Type`,`Crop Type`)) %>%
cbind(dummies)
View(train)
View(train)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softprob"),
nrounds = 10)
View(train_y)
is.na(train_y)
sum(is.na(train_y))
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softprob"),
nrounds = 10)
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softprob",
eval_metric = "mlogloss"),
nrounds = 10)
as.numeric(as.factor(train$`Fertilizer Name`)) - 1
train$`Fertilizer Name` <-
as.numeric(as.factor(train$`Fertilizer Name`)) - 1
View(train)
train_x = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
train_y = train %>%
filter(fold %in% c(1:4)) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
test_x = train %>%
filter(fold == 5) %>%
dplyr::select(!c(id, fold, `Fertilizer Name`)) %>%
as.matrix()
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softprob"),
nrounds = 10)
yhat = predict(mod, test_x, validate_features = TRUE)
yhat
mod
mod <- xgboost(data = train_x,
label = train_y,
params = list(num_class=7,
objective = "multi:softmax"),
nrounds = 10)
yhat = predict(mod, test_x, validate_features = TRUE)
test_y = train %>%
filter(fold == 5) %>%
dplyr::select(`Fertilizer Name`) %>%
as.matrix()
View(test_y)
nrow(yhat)
length(yhat)
sum(yhat==test_y)/length(yhat)
cbind(yhat, test_y)
